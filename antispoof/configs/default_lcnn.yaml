logger:
  _target_: antispoof.logger.logger.get_logger
  experiment_name: ${name}
  name: train
  save_dir: ${trainer.save_dir}
spec_processing:
  _target_: antispoof.frontend.STFT2
  n_fft: 512
  window_length: 320
  hop_length: 160
data:
  train:
    _target_: torch.utils.data.DataLoader
    batch_size: 64
    num_workers: 10
    shuffle: True
    drop_last: False
    collate_fn: 
      _target_: antispoof.collate_fn.get_collate_fn
    dataset:
      _target_: antispoof.datasets.LADataset
      part: train
      sr: ${trainer.sr}
      spec_processing: ${spec_processing}
      spec_segment_length: 750
  test:
    _target_: torch.utils.data.DataLoader
    batch_size: 64
    num_workers: 10
    shuffle: False
    drop_last: False
    collate_fn: 
      _target_: antispoof.collate_fn.get_collate_fn
    dataset:
      _target_: antispoof.datasets.LADataset
      part: eval
      sr: ${trainer.sr}
      spec_processing: ${spec_processing}
      spec_segment_length: 750
metrics:
  - _target_: antispoof.metric.Accuracy
device: 
  _target_: torch.device
  device: "cuda:0"
kaiman: False
arch:
  _target_: antispoof.model.LightCNNTrim
  backbone:
    _target_: antispoof.model.LightCNNBackbone
    dropout: 0.75
    kaiman: ${kaiman}
  head:
    _target_: antispoof.model.LinearHead
    ifc: 23552
    dropout: 0.75
    kaiman: ${kaiman}
  softmax:
    _target_: antispoof.model.NormalSoftmax
lr_scheduler:
  _target_: antispoof.scheduler.getExponentialScheduler
  gamma: 0.9
optimizer:
  _target_: torch.optim.Adam
  betas:
    - 0.9
    - 0.999
  eps: 0.00000001
  lr: 0.0003
trainer:
  _target_: antispoof.trainer.Trainer
  epochs: 15
  save_period: 1
  verbosity: 2
  save_dir: saved/${name}/
  visualize: wandb
  wandb_project: antispoof_project
  monitor: min test_eer
  sr: 16000